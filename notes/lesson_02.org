#+TITLE: AI for NLP -- Lesson 1

* Chapt1: Introduction

* Chapt2: AI Paradigm

** 1. Rule Based

#+BEGIN_QUOTE
基于固定范式去生成语言
#+END_QUOTE

*** 1. 简单语言范式与生成

1. 简单语言范式描述

   #+BEGIN_SRC python
simple_grammar = """
sentence => noun_phrase verb_phrase
noun_phrase => Article Adj* noun
Adj* => null | Adj Adj*
verb_phrase => verb noun_phrase
Article => ⼀个 | 这个
noun => ⼥⼈ | 篮球 | 桌⼦ | ⼩猫
verb => 看着 | 坐在 | 听着 | 看⻅
Adj => 蓝⾊的 | 好看的 | ⼩⼩的
"""
   #+END_SRC

2. 简单的 solution: 按照 grammar 描述，对每个词性分别生成相应词汇，最后组合。
   - 缺点： *当语法更改后，函数组合需要重新修改*

   #+BEGIN_SRC python
import random

def adj():
    """
    根据固定的 adj. 列表生成随机一个 adj.
    """
    return random.choice("蓝色的 | 好看的 | 小小的".split("|")).split()[0]

def adj_star(depth=1):
    """
    生成随机的 adj. 组合
    DONE: 设置 adj_star 为递归函数，并设置最大递归深度
    """
    # 设置递归最大深度为 3
    if depth >= 3:
        return adj()
    return random.choice(["", adj() + adj_star(depth+1)])
   #+END_SRC

*** 2. generative 语言范式的描述与生成

1. grammar 描述

   #+BEGIN_SRC python
adj_grammar = """
Adj* => null | Adj Adj*
Adj => 蓝色的 | 小小的 | 好看的
"""
   #+END_SRC

2. 生成新的 grammar 结构

   #+BEGIN_SRC python
# 生成 grammar 数据结构
grammar = {}

for line in adj_grammar.split("\n"):
    if not line.strip(): continue
    exp, stmt = line.split("=>")
    grammar[exp.split()[0]] = [s.split() for s in stmt.split("|")]
   #+END_SRC

3. 终止符与可扩展的：当遇到终止符，就返回；如遇到可扩展的，则继续扩展，直至遇到终止符

   #+BEGIN_SRC python
def generate(gram, target):
    if target not in gram: return target
    return "".join(generate(gram, t) for t in random.choice(gram[target]))
   #+END_SRC

*** 3. generative 方式生成语言

1. simple grammar 的处理

   - ~create_grammar(gram, split="|")~ 函数: 生成新的语法数据结构

     #+BEGIN_SRC python
def create_grammar(gram, outer_delimiter="=>", inner_delimiter="|", line_delimiter="\n"):
    grammar = {}
    for line in gram.split(line_delimiter):
        if not line.strip():
            continue
        exp, stmt = line.split(outer_delimiter)
        grammar[exp.split()[0]] = [s.split() for s in stmt.split(inner_delimiter)]
    return grammar
     #+END_SRC

   - ~generate(gram, target)~: 生成新的语句

     #+BEGIN_SRC python
def generate(gram, target):
    if target not in gram: return target
    expand = [generate(gram, t) for t in random.choice(gram[target])]
    return ''.join([e if e != '/n' else '\n' for e in expand if e != "null"])
     #+END_SRC

   - 生成示例语句

     #+BEGIN_SRC python
grammar = create_grammer(simple_grammer)
generate(grammar, "sentence")
     #+END_SRC

     #+RESULTS:
     : '⼀个蓝⾊的篮球看⻅这个⼩⼩的篮球'

2. 西部世界语言生成示例

   #+BEGIN_SRC python
# 人类语言
human = """
human = 自己 寻找 活动
自己 = 我 | 俺 | 我们
寻找 = 看看 | 找找 | 想找点
活动 = 乐子 | 玩的
"""

# 接待员语言
host = """
host = 寒暄 报数 询问 业务相关 结尾
报数 = 我是 数字 号 ，
数字 = 单个数字 | 数字 单个数字
单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
寒暄 = 称谓 打招呼 | 打招呼
称谓 = 人称 ，
人称 = 先生 | 女士 | 小朋友
打招呼 = 你好， | 您好，
询问 = 请问您要 | 您需要
业务相关 = 玩玩 具体业务
具体业务 = 喝酒 | 打牌 | 打猎 | 赌博
结尾 = 吗？"""

generate(create_grammar("host", "="), target="host")
   #+END_SRC

   #+RESULTS:
   : '您好我是9号，请问您要玩玩赌博吗？'

*** 4. Data Driven

#+BEGIN_QUOTE
我们希望：当数据数据时，我们的程序不需要重写，generalization AI?
#+END_QUOTE

1. 程序语言生成

   #+BEGIN_SRC python
programming = """
stmt => if_exp | while_exp | assignment
assignment => var = var
if_exp => if ( var ) { /n .... stmt }
while_exp => while ( var ) { /n .... stmt }
var => chars number
chars => a | b | c | d | e | f | g
number => 1 | 2 | 3
"""
generate(create_grammar(programming), "stmt")
   #+END_SRC
  
** 2. Probability Based

*** 1. 概率模型提出

#+BEGIN_QUOTE
语言模型产生不同的语句，如何判断哪一句语句更好，此时，需要概率模型对语句进行判断。
#+END_QUOTE

1. 自动机: Hard to represent sentence as a tree.
2. Language Model

   \begin{aligned}
     Language Model(String) &= Probability(String) \in (0, 1)\\
     Pr(w_1, w_2, w_3, w_4) &= Pr(w_1|w_2, w_3, w_4) \times Pr(w_2, w_3, w_4)\\
     &=Pr(w_1|w_2, w_3, w_4) \times Pr(w_2|w_3, w_4) \times Pr(w_3, w_4) \\
     &=Pr(w_1|w_2, w_3, w_4) \times Pr(w_2|w_3, w_4) \times Pr(w_3|w_4) \times Pr(w_4)
   \end{aligned}

3. How to calculate probability of $P(w_1|w_2, w_3, w_4)$

   \begin{aligned}
     P(w_{1}|w_{2}, w_{3}, w_{4}) &= \frac{P(w_{1}, w_{2}, w_{3}, w_{4})}{P(w_{2}, w_{3}, w_{4})}\\
     &=\frac{Counter(w_{1}, w_{2}, w_{3}, w_{4})}{Counter(w_{2}, w_{3}, w_{4})}
   \end{aligned}

4. 假设：每个单词出现概率仅于其之后的第一个字相关，于是，就有

   \begin{equation}
     Pr(w_{1}, w_{2}, w_{3}, w_{4}) \sim P(w_{1}|w_{2}) \times P(w_{2}|w_{3}) \times P(w_{3}|w_{4}) \times P(w_{4})
   \end{equation}

5. N-gram 模型

   课堂上的提法与传统的 N-gram 模型描述正好相反，这里附上 N-gram 相关的描述

   - N-gram 概率模型

     \begin{aligned}
         Pr(W) &= Pr(w_1, w_2, w_3, w_4\ldots,)\\
         &= Pr(w_1)\times Pr(w_2, w_3, w_4, \ldots|w_1)\\
         &= Pr(w_1)\times Pr(w_2|w_1) \times Pr(w_3, w_4, \ldots|w_1, w_2)\\
         \ldots
     \end{aligned}

   - Unigram model: 假设每个单词出现的概率是独立的，与其他单词无关，即

     \begin{equation}
       Pr(w_{i}|w_{i-1}, w_{i-2},\ldots,w_{1}) = Pr(w_{i})
     \end{equation}

   - Bigram model: 假设每个单词出现仅与其之前一个单词有关

     \begin{equation}
       Pr(w_{i}|w_{i-1}, w_{i-2},\ldots,w_{1}) = Pr(w_{i}|w_{i-1})
     \end{equation}
    
*** 2. 新闻文本分析

1. 文本预处理

   - 读取文件

     #+BEGIN_SRC python
import pandas as pd

df = pd.read_csv("sqlResult_1558435.csv", encoding="gb18030")
articles = df["content"].tolist()
     #+END_SRC

   - 特殊符号处理

     #+BEGIN_SRC python
import re

def token(string):
    """
    将 string 中普通词汇取出
    """
    return re.findall("\w+", string)

# be careful: token 返回的是一个 list
articles_clean = ["".join(token(str(a))) for a in articles]
     #+END_SRC

   - 结巴分词处理

     #+BEGIN_SRC python
import jieba
def cut(string):
    """
    利用结巴分词对 string 进行分词，并返回词汇列表
    """
    return list(jieba.cut(string))

# 所有文章的中文词汇表
article_words = [cut(string) for string in articles_clean]
     #+END_SRC

   - 降维处理

     #+BEGIN_SRC python
from functools import reduce
from operator import add

token_1g = reduce(add, article_words)
     #+END_SRC

   - 针对大文件处理

     #+BEGIN_SRC python
"""
如果文件太大，可以先将内容存为文件再处理
"""
import pandas as pd
import re
from collections import Counter
import jieba

# 1. 读取文件
articles = pd.read_csv("sqlResult_1558435.csv", encoding='gb18030')['content'].tolist()

# 2. 处理特殊符号
articles_clean = ["".join(re.findall("\w+", str(article))) for article in articles]

# 3. 将符号处理过的内容存为文件
with open("articles_9k.txt", "w") as f:
    for article in articles_clean:
        f.write(article+"\n")

# 4. 分词
# jieba 分词出来的是 generator, 需要转换为 list
def cut(string):
    return list(jieba.cut(string))

token_1g = []
# 逐行读取
for i, line in enumerate(open("articles_9k.txt")):
    if i % 100 == 0:
        print(i)
    token_1g += cut(line)

# 5. 降维
words_1g_count = Counter(token_1g)
# 查看出现词组最多的前 10 个
print(words_1g_count.most_common(10))
     #+END_SRC

   - 可视化

     #+BEGIN_SRC python
import matplotlib.pyplot as plt

frequencies = [v for k, v in words_1g_count.most_common(100)]
x = [i for i in range(100)]

plt.plot(x, frequencies)
     #+END_SRC

2. 概率计算 (按照 2-gram 模型定义)

   - 单独词汇概率

     #+BEGIN_SRC python
def prob_single(word):
    """
    加入对于非语料库词汇概率计算
    利用 log 函数避免结果越界
    """
    if word in words_1g_count:
        return words_1g_count[word] / len(token_1g)
    else:
        return -math.log(words_1g_count[word]/len(token_1g))
     #+END_SRC

   - 连续两个词汇的分布

     #+BEGIN_SRC python
token_2g = ["".join(token_1g[i:i+2] for i in range(len(token_1g) - 2))]
words_2_counter = Counter(token_2g)
     #+END_SRC

   - 连续两个词出现概率

     #+BEGIN_SRC python
def prob_dual(word_1, word_2):
    if word_1 + word_2 in words_2_count:
        return -math.log(words_2_count[word_1+word_2] / len(token_2g))
    else:
        return prob_single(word_2) + prob_single(word_2)
     #+END_SRC

   - 整句话出现概率

     #+BEGIN_SRC python
def get_probability(sentence):
    words = cut(sentence)

    sentence_pro = 0.
    for i, word in enumerate(words[:-1]):
        next_ = words[i+1]
        prob_2_gram = prob_dual(word, next_)

        sentence_pro += prob_2_gram
    return sentence_pro
     #+END_SRC

3. 注意点

   #+BEGIN_QUOTE
   当词库很大的时候，某个词汇出现的概率很小，如果某句话很长，
   那么概率连乘的后果就是概率无限趋近于 0，
   超出浮点表示范围，此时，可以用 $-\log$ 函数来对概率进行处理，避免越界。
   #+END_QUOTE

** DONE 3. Problem Solving: Search Based

*** 1. 自动化处理问题

#+BEGIN_QUOTE
类似路径规划，决策问题等，解决方式可以通过类似 Search Based 方式来进行解决，比较典型的问题是地图搜索
问题。
#+END_QUOTE

*** 2. 地图路径规划

1. 初始城市经纬度

   #+BEGIN_SRC python
coordination_source = """
{name:'兰州', geoCoord:[103.73, 36.03]},
{name:'嘉峪关', geoCoord:[98.17, 39.47]},
{name:'西宁', geoCoord:[101.74, 36.56]},
{name:'成都', geoCoord:[104.06, 30.67]},
{name:'石家庄', geoCoord:[114.48, 38.03]},
{name:'拉萨', geoCoord:[102.73, 25.04]},
{name:'贵阳', geoCoord:[106.71, 26.57]},
{name:'武汉', geoCoord:[114.31, 30.52]},
{name:'郑州', geoCoord:[113.65, 34.76]},
{name:'济南', geoCoord:[117, 36.65]},
{name:'南京', geoCoord:[118.78, 32.04]},
{name:'合肥', geoCoord:[117.27, 31.86]},
{name:'杭州', geoCoord:[120.19, 30.26]},
{name:'南昌', geoCoord:[115.89, 28.68]},
{name:'福州', geoCoord:[119.3, 26.08]},
{name:'广州', geoCoord:[113.23, 23.16]},
{name:'长沙', geoCoord:[113, 28.21]},
//{name:'海口', geoCoord:[110.35, 20.02]},
{name:'沈阳', geoCoord:[123.38, 41.8]},
{name:'长春', geoCoord:[125.35, 43.88]},
{name:'哈尔滨', geoCoord:[126.63, 45.75]},
{name:'太原', geoCoord:[112.53, 37.87]},
{name:'西安', geoCoord:[108.95, 34.27]},
//{name:'台湾', geoCoord:[121.30, 25.03]},
{name:'北京', geoCoord:[116.46, 39.92]},
{name:'上海', geoCoord:[121.48, 31.22]},
{name:'重庆', geoCoord:[106.54, 29.59]},
{name:'天津', geoCoord:[117.2, 39.13]},
{name:'呼和浩特', geoCoord:[111.65, 40.82]},
{name:'南宁', geoCoord:[108.33, 22.84]},
//{name:'西藏', geoCoord:[91.11, 29.97]},
{name:'银川', geoCoord:[106.27, 38.47]},
{name:'乌鲁木齐', geoCoord:[87.68, 43.77]},
{name:'香港', geoCoord:[114.17, 22.28]},
{name:'澳门', geoCoord:[113.54, 22.19]}
"""
   #+END_SRC

2. 字符串正则匹配与分割

   #+BEGIN_SRC python
import re

city_location = {}
for line in coordination_source.split("\n"):
    # 忽略注释语句
    if line.startswith("//"): continue
    # 忽略空白语句
    if not line.strip(): continue

    city = line.findall("name:'(\w+)'", line)[0]

    x_y = line.findall("Coord:\[(\d+\.?\d+?),\s(\d+\.?\d+?)\]", line)[0]
    x_y = tuple(map(float, x_y))
    city_location[city] = x_y

   #+END_SRC

3. 不同城市之间距离计算公式

   #+BEGIN_SRC python
from geopy.distance import geodestic

def get_city_distance(city_1: str, city_2: str):
    """
    获取两个城市之间距离，单位 Km
    """
    return geodestick(city_location[city_1], city_location[city_2]) / 1000.
   #+END_SRC

4. 不同城市结构显示

   #+BEGIN_SRC python
import matplotlib.pyplot as plt
import networkx as nx

cities = list(city_location.keys())
city_graph = nx.Graph()
city_graph.add_nodes_from(cities)
nx.draw(city_graph, city_location, with_labels=True, node_size=10)
   #+END_SRC

5. 设置城市拓扑结构

   #+BEGIN_SRC python
threshold = 700
from collections import defaultdict

cities_connection = defaultdict(list)

for c1 in cities:
    for c2 in cities:
        if c1 == c2: continue
        if get_city_distance(c1, c2) < threshold:
            cities_connection[c1].append(c2)

city_connection_graph = nx.Graph(cities_connection)
nx.draw(cities_connection_graph, city_location, with_labels=True, node_size=10)
   #+END_SRC

6. 路径搜索

   - 深度优先

     #+BEGIN_QUOTE
     优先搜索完某一分支后，继续搜索下一条分支，直至找到正确路径
     #+END_QUOTE

   - 广度优先

     #+BEGIN_QUOTE
     优先搜索同级结点，搜索结束后，继续搜索下一级结点，直至找到正确路径
     #+END_QUOTE

   - 代码

     #+BEGIN_SRC python
def is_goal(destination):
    def _wrap(current_path):
        return current_path[-1] == destination:
    return _wrap

def search(graph: dict, start: str, is_goal, search_strategy):
    """
    根据 graph 实现路径搜索
    :param graph: 结点直接连接结构
    :param start: 起始点
    :param is_goal: 函数，输入路径，判断路径是否正确
    :param search_strategy: 搜索方式
    """
    pathes = [[start]]
    seen = set() # 已经判断过的结点

    while(pathes):
        path = pathes.pop(0)
        frontier = path[-1]

        if frontier in seen:
            continue

        successors = graph[frontier]

        for city in successors:
            if city in path: continue
            new_path = path + [city]
            pathes.append(new_path)

            if is_goal(new_path): return new_path

        seen.add(frontier)
        pathes = search_strategy(pathes)

def sort_path(cmp_func, beam=-1):
    def _sorted(pathes):
        return sorted(pathes, key=cmp_func)[:beam]
    return _sorted
     #+END_SRC

** TODO 4. Mathematical or Analytic Based
** TODO 5. Machine Learning (deep learning) Based
